---
title: "Stat 341 -- PS 15"
author: "Ben Steves"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  pdf_document: default
  html_document: default
  word_document: default
---

```{r, setup, include = FALSE, message=FALSE}
# load packages that are going to be used

library(dagitty)
dagitty("dag{}")


library(mosaic)      # this loads ggformula (for plotting), etc. too
library(fastR2)      # some data sets
library(pander)      # nicely formatted tables with pander()
library(knitr)       # so you can use kable()
library(patchwork)   # for combining plots

# part of tidyverse for data wrangling
library(dplyr)
library(tidyr)
library(purrr)

# several packages for bayesian stuff -- more to come later
library(rethinking)  # related to text
library(tidybayes)    
library(bayesplot)
library(MASS)
library(CalvinBayes)


# Some customization. You can alter or delete as desired (if you know what you are doing).

theme_set(theme_bw())     # change theme for ggplot2/ggformula

knitr::opts_chunk$set(
  tidy = FALSE,     # display code as typed (rather than reformatted)
  fig.width = 4,    # adjust this to make figures wider or narrower
  fig.height = 2.5, # adjust this to make figures taller or shorrter
  size = "small")   # slightly smaller font for code
```


<!-- A few math abbreviations -->

\newcommand{\Prob}{\operatorname{Pr}}
\newcommand{\Binom}{\operatorname{Binom}}
\newcommand{\Unif}{\operatorname{Unif}}
\newcommand{\Triangle}{\operatorname{Triangle}}
\newcommand{\Norm}{\operatorname{Norm}}
\newcommand{\Beta}{\operatorname{Beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\SD}{\operatorname{SD}}

<!-- Put your work below here.  Put text in text chunks, code in R chunks. -->

# 11e1

```{r}
p <- 0.35
odds1 <- p / (1-p)
log(odds1)
```

# 11e2

```{r}
ilogit(3.2)
```

# 11h2

### a)

```{r}
data(eagles); ?eagles
```

```{r}
eagles <- eagles %>%
  mutate(P_bin = ifelse(P == "L", 1, 0),
         A_bin = ifelse(A == "A", 1, 0),
         V_bin = ifelse(V == "L", 1, 0),
         prob = y/n)
```

```{r}
q11h2a <- quap(
  data = eagles,
  alist(
    y ~ dbinom(n, p),
    logit(p) <- a + bp*P_bin + ba*A_bin + bv*V_bin,
    a ~ dnorm(0, 1.5),
    c(bp, ba, bv) ~ dnorm(0, 0.5)
  )
)

```

```{r results = FALSE}
u11h2a <- ulam(
    data = eagles,
  alist(
    y ~ dbinom(n, p),
    logit(p) <- a + bp*P_bin + ba*A_bin + bv*V_bin,
    a ~ dnorm(0, 1.5),
    c(bp, ba, bv) ~ dnorm(0, 0.5)
  ),
  chains = 4, iter = 4000, warmup = 1000, cores = 4,
  refresh = 0, log_lik = TRUE
)

```

```{r fig.height = 4, fig.width = 6}
plot(coeftab(u11h2a, q11h2a))
```

The quap model is comparable to the ulam model, so there doesn't seem to be an issue with quadratic approximation here.

### b)

```{r}
set.seed(974)
Post11h2 <-
  extract.samples(q11h2a)
head(Post11h2)
```

```{r fig.width = 6, fig.height = 4}
postcheck(q11h2a)
```


```{r}
Post11h2 <- Post11h2 %>%
  mutate(p = ilogit(a))
```

```{r}
set.seed(974)
Avg11h2 <-
  link(q11h2a) %>%
  apply(2, mean_hdi, .width = 0.89) %>%
  bind_rows() %>%
  bind_cols(eagles)
Avg11h2
```

```{r fig.width = 6, fig.height = 4}
Avg11h2$p_a_v <- paste(Avg11h2$P, Avg11h2$A, Avg11h2$V)
Avg11h2 <- Avg11h2 %>%
  mutate(y = y...1)
eagles$p_a_v <- paste(eagles$P, eagles$A, eagles$V)
gf_point(y ~ p_a_v, data = Avg11h2) %>%
  gf_errorbar(ymin+ymax ~ p_a_v, data = Avg11h2) %>%
  gf_point(prob ~ p_a_v, data = eagles, color = "red", size = 2)
```


```{r}
set.seed(974)
Ind11h2 <-
  sim(q11h2a) %>%
  apply(2, mean_hdi, .width = 0.89) %>%
  bind_rows() %>%
  #had to do some filtering because sim was giving me multiple repeated rows
  slice(1, 3:4, 6:9, 11) %>%
  bind_cols(eagles)

Ind11h2
```

```{r fig.width = 6, fig.height = 4}
Ind11h2$p_a_v <- paste(Ind11h2$P, Ind11h2$A, Ind11h2$V)
Ind11h2 <- Ind11h2 %>%
  mutate(y = y...1)
gf_point(y ~ p_a_v, data = Ind11h2) %>%
  gf_errorbar(ymin+ymax ~ p_a_v, data = Ind11h2) %>%
  gf_point(y ~ p_a_v, data = eagles, color = "red", size = 2)
```

The model's predictions for the value of y seem a little better than the models predictions for the probability of successful swoop attempts, though the probability plot may be more helpful as it gives us more of an intuition on how often swoop attempts happened, in contrast to the y-value plot which only predicts a raw number.

### c)

```{r}
q11h2c <-
  quap(
    data = eagles,
    alist(
      y ~ dbinom(n, p),
      logit(p) <- a + bp*P_bin + ba*A_bin + bv*V_bin + bap*A_bin*P_bin,
      a ~ dnorm(0, 1.5),
      c(bp, ba, bv, bap) ~ dnorm(0, 0.5)
    )
  )
```

```{r fig.width = 6, fig.height = 4}
compare(q11h2a, q11h2c) %>% pander()
compare(q11h2a, q11h2c) %>% plot()
```

The predictions are slightly better for the original model without the interaction based on WAIC, though this isn't by a ton. The difference line's bounds don't pass the vertical line either, so it doesn't seem like the model with an interaction is as good as the model without one. 

# 11h4

```{r}
data("NWOGrants")
```

```{r}
dag11h4 <- 
  dagitty("dag { 
  Grants <- Gender; Grants <- Discipline <- Gender}" )
drawdag(dag11h4)

```

```{r}
NWOGrants <- NWOGrants %>%
  mutate(g = ifelse(gender == "m", 1, 2),
         d = as.integer(discipline),
         prob = awards/applications)

mean(NWOGrants$prob) 
p <- 0.1903
odds2 <- p / (1-p)
log(odds2)
```


```{r results = FALSE}
set.seed(133)
m11h4a <- ulam(
  data = NWOGrants,
  alist(
    awards ~ dbinom(applications, p),
    logit(p) <- a[g],
    a[g] ~ dnorm(-1, 0.5)
  ),
  chains = 4, iter = 4000, warmup = 1000, cores = 4,
  refresh = 0, log_lik = TRUE
)
```


```{r results=FALSE}
set.seed(133)
m11h4b <- ulam(
  data = NWOGrants,
  alist(
    awards ~ dbinom(applications, p),
    logit(p) <- a_g[g] + a_d[d],
    a_g[g] ~ dnorm(-1, 0.5),
    a_d[d] ~ dnorm(-1, 0.5)
  ),
  chains = 4, iter = 4000, warmup = 1000, cores = 4,
  refresh = 0, log_lik = TRUE
)
```

```{r}
#m11h4a results
precis(m11h4a, depth = 2)
ilogit(c(-1.53, -1.73))
0.17799 - 0.15059 #p difference
-1.73 - - 1.53 #logit difference

#males prob hdi
ilogit(c(-1.63, -1.42))

#females prob hdi
ilogit(c(-1.85, -1.6))
```

From the model not containing discipline, it is predicting men to have slightly higher grant acceptance rates than women


```{r fig.width = 7, fig.height = 4}
#m11h4b results
precis(m11h4b, depth = 2)
```

```{r}
ilogit(c(-.54, -.69))
```

Interestingly the probability for a grant increases quite a bit compared to the last model. 

```{r results = FALSE}
set.seed(133)
Avg11h4 <-
  link(m11h4b) %>%
  apply(2, mean_hdi, .width = 0.89) %>%
  bind_rows() %>%
  bind_cols(NWOGrants)
```

```{r fig.height = 6, fig.width = 4}
Avg11h4$g_d <- paste(Avg11h4$g, Avg11h4$d)
NWOGrants$g_d <- paste(NWOGrants$g, NWOGrants$d)
gf_point(y ~ g | d, data = Avg11h4) %>%
  gf_errorbar(ymin+ymax ~ g | d, data = Avg11h4) %>%
  gf_point(prob ~ g| d, data = NWOGrants, color = "red", size = 2)
```

On the left in the plots is men, on the right is women, with facets for each discipline. In looking at the predictions vs the original data, for the most part the model predicts men to get grants at slightly higher rates then women, but it is much less than the last model accounted for, as the black dots (the predicted means) are for the most part, closer together than the actual data, symbolized by the red dots. The grant award rate differs by discipline, but in the disciplines there are not really any massive differences between women and men. When we didn't include discipline, our assumptions would be that women get accepted in applying for grants less than men, but this masks the effect of discipline. Including discipline shows that the rates are different for both men and women in different departments. 